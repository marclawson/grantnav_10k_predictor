{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection: Text (Description of Grant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Research Question**\n",
    "    \n",
    "Is it possible to predict grant value from description keywords, location-based data and/or other characteristics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:48:57.979782Z",
     "iopub.status.busy": "2021-06-03T05:48:57.979551Z",
     "iopub.status.idle": "2021-06-03T05:49:00.462660Z",
     "shell.execute_reply": "2021-06-03T05:49:00.461888Z",
     "shell.execute_reply.started": "2021-06-03T05:48:57.979728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/marclawson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#system\n",
    "import os\n",
    "import sys\n",
    "from os.path import join as pj\n",
    "module_path = os.path.abspath(pj('..','..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "import random\n",
    "from operator import itemgetter\n",
    "\n",
    "# viz\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)\n",
    "plt.style.use('bmh')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import missingno as msno\n",
    "from tqdm import tqdm\n",
    "\n",
    "# configurations\n",
    "from pathlib import Path\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import configparser\n",
    "\n",
    "#stats\n",
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# utils\n",
    "from src.d00_utils import print_helper_functions as phf\n",
    "\n",
    "# ipython\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# type annotations\n",
    "from typing import List, Set, Dict, Tuple, Optional\n",
    "from collections.abc import Iterable\n",
    "\n",
    "# machine learning\n",
    "from scratch.deep_learning import Tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NLP\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:49:00.464815Z",
     "iopub.status.busy": "2021-06-03T05:49:00.464526Z",
     "iopub.status.idle": "2021-06-03T05:49:00.528718Z",
     "shell.execute_reply": "2021-06-03T05:49:00.527556Z",
     "shell.execute_reply.started": "2021-06-03T05:49:00.464791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/marclawson/repositories/grantnav_10k_predictor\n"
     ]
    }
   ],
   "source": [
    "# not used in this stub but often useful for finding various files\n",
    "project_dir = Path().resolve().parents[1]\n",
    "print(project_dir)\n",
    "\n",
    "# find .env automagically by walking up directories until it's found, then\n",
    "# load up the .env entries as environment variables\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "raw_dir = pj(project_dir, 'data', os.environ.get('RAW_DIR'))\n",
    "interim_dir = pj(project_dir, 'data', os.environ.get('INTERIM_DIR'))\n",
    "processed_dir = pj(project_dir, 'data', os.environ.get('PROCESSED_DIR'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Word Vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to see what words or small groups of words appear frequently.  We can compare both count and tfidf vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:49:00.530145Z",
     "iopub.status.busy": "2021-06-03T05:49:00.529727Z",
     "iopub.status.idle": "2021-06-03T05:49:03.324822Z",
     "shell.execute_reply": "2021-06-03T05:49:03.323972Z",
     "shell.execute_reply.started": "2021-06-03T05:49:00.530107Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(pj(interim_dir,'grantnav_data.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:49:03.327343Z",
     "iopub.status.busy": "2021-06-03T05:49:03.327117Z",
     "iopub.status.idle": "2021-06-03T05:49:03.386096Z",
     "shell.execute_reply": "2021-06-03T05:49:03.385571Z",
     "shell.execute_reply.started": "2021-06-03T05:49:03.327318Z"
    }
   },
   "outputs": [],
   "source": [
    "pt_definitions = {'CC': 'coordinating conjunction',\n",
    "'CD': 'cardinal digit',\n",
    "'DT': 'determiner',\n",
    "'EX': 'existential there',\n",
    "'FW': 'foreign word',\n",
    "'IN': 'preposition/subordinating conjunction',\n",
    "'JJ': 'This NLTK POS Tag is an adjective (large)',\n",
    "'JJR': 'adjective, comparative (larger)',\n",
    "'JJS': 'adjective, superlative (largest)',\n",
    "'LS': 'list market',\n",
    "'MD': 'modal (could, will)',\n",
    "'NN': 'noun, singular (cat, tree)',\n",
    "'NNS': 'noun plural (desks)',\n",
    "'NNP': 'proper noun, singular (sarah)',\n",
    "'NNPS': 'proper noun, plural (indians or americans)',\n",
    "'PDT': 'predeterminer (all, both, half)',\n",
    "'POS': \"possessive ending (parent\\ 's)\",\n",
    "'PRP': 'personal pronoun (hers, herself, him,himself)',\n",
    "'PRP$': 'possessive pronoun (her, his, mine, my, our )',\n",
    "'RB': 'adverb (occasionally, swiftly)',\n",
    "'RBR': 'adverb, comparative (greater)',\n",
    "'RBS': 'adverb, superlative (biggest)',\n",
    "'RP': 'particle (about)',\n",
    "'TO': 'infinite marker (to)',\n",
    "'UH': 'interjection (goodbye)',\n",
    "'VB': 'verb (ask)',\n",
    "'VBG': 'verb gerund (judging)',\n",
    "'VBD': 'verb past tense (pleaded)',\n",
    "'VBN': 'verb past participle (reunified)',\n",
    "'VBP': 'verb, present tense not 3rd person singular(wrap)',\n",
    "'VBZ': 'verb, present tense with 3rd person singular (bases)',\n",
    "'WDT': 'wh-determiner (that, what)',\n",
    "'WP': 'wh- pronoun (who)',\n",
    "'WRB': 'wh- adverb (how)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:49:03.387987Z",
     "iopub.status.busy": "2021-06-03T05:49:03.387802Z",
     "iopub.status.idle": "2021-06-03T05:49:03.444367Z",
     "shell.execute_reply": "2021-06-03T05:49:03.443546Z",
     "shell.execute_reply.started": "2021-06-03T05:49:03.387968Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_document_grammars(grantrow: List[Tuple[str, str]], grammar: List[str] = ['NN']) -> List[str]:\n",
    "    \"\"\"Returns list of all specified grammar types within text\"\"\"\n",
    "    text = nltk.word_tokenize(grantrow.lower())\n",
    "    pos_tagged = nltk.pos_tag(text)\n",
    "    document = []\n",
    "    for g in grammar:\n",
    "        grammar_in_doc = [_[0] for _ in \\\n",
    "                    filter(lambda x:x[1].startswith(g), \\\n",
    "                           pos_tagged)]\n",
    "        document.extend(grammar_in_doc)\n",
    "    return list(set(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:49:03.446192Z",
     "iopub.status.busy": "2021-06-03T05:49:03.445678Z",
     "iopub.status.idle": "2021-06-03T05:49:05.025357Z",
     "shell.execute_reply": "2021-06-03T05:49:05.024652Z",
     "shell.execute_reply.started": "2021-06-03T05:49:03.446171Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "for row in data['description'].sample(1000):\n",
    "    documents.append(create_document_grammars(row, grammar=['NN','RBS']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:49:05.026934Z",
     "iopub.status.busy": "2021-06-03T05:49:05.026624Z",
     "iopub.status.idle": "2021-06-03T05:49:05.083952Z",
     "shell.execute_reply": "2021-06-03T05:49:05.083139Z",
     "shell.execute_reply.started": "2021-06-03T05:49:05.026913Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [' '.join(_) for _ in documents]\n",
    "\n",
    "def vectorizer_word_counts(corpus: List[str]) -> pd.DataFrame:\n",
    "\n",
    "    vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "    vec.fit(corpus)\n",
    "    vec_mat = vec.transform(corpus)\n",
    "\n",
    "    def word_frequency_sorter(words, counts):\n",
    "        sort_result = sorted(zip(words, list(counts)),\n",
    "                             key=itemgetter(1), reverse=True)\n",
    "        return sort_result\n",
    "\n",
    "    def word_counter(mat):\n",
    "        count = np.array(mat.sum(axis=0))[0]\n",
    "        return count\n",
    "\n",
    "    words = vec.get_feature_names()\n",
    "    _sum = word_counter(vec_mat) \n",
    "    df_word_frequency = pd.DataFrame(word_frequency_sorter(\n",
    "        words, _sum), columns=['word', 'frequency'])\n",
    "    df_word_frequency.set_index('word', inplace=True)\n",
    "    return df_word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:49:05.085403Z",
     "iopub.status.busy": "2021-06-03T05:49:05.085208Z",
     "iopub.status.idle": "2021-06-03T05:49:05.236053Z",
     "shell.execute_reply": "2021-06-03T05:49:05.235349Z",
     "shell.execute_reply.started": "2021-06-03T05:49:05.085382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>project</th>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funding</th>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grant</th>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>community</th>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>costs</th>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>programme</th>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activities</th>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>children</th>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            frequency\n",
       "word                 \n",
       "project           288\n",
       "funding           263\n",
       "people            182\n",
       "grant             174\n",
       "community         169\n",
       "group             149\n",
       "costs             126\n",
       "programme         123\n",
       "activities        105\n",
       "children           99"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phf.print_full(vectorizer_word_counts(corpus).iloc[:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:49:05.237156Z",
     "iopub.status.busy": "2021-06-03T05:49:05.236984Z",
     "iopub.status.idle": "2021-06-03T05:49:05.290185Z",
     "shell.execute_reply": "2021-06-03T05:49:05.289533Z",
     "shell.execute_reply.started": "2021-06-03T05:49:05.237137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525023, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:49:05.291491Z",
     "iopub.status.busy": "2021-06-03T05:49:05.291231Z",
     "iopub.status.idle": "2021-06-03T05:49:05.423996Z",
     "shell.execute_reply": "2021-06-03T05:49:05.422815Z",
     "shell.execute_reply.started": "2021-06-03T05:49:05.291469Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = data['description']#.sample(10000)\n",
    "\n",
    "corpus_train, corpus_test = train_test_split(corpus, train_size=0.8, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:49:05.428478Z",
     "iopub.status.busy": "2021-06-03T05:49:05.428237Z",
     "iopub.status.idle": "2021-06-03T05:50:08.966825Z",
     "shell.execute_reply": "2021-06-03T05:50:08.965064Z",
     "shell.execute_reply.started": "2021-06-03T05:49:05.428453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=5000, stop_words='english',\n",
       "                strip_accents='unicode', token_pattern='\\\\w+')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nonzero entries:\n",
      "7194440\n",
      "Highest count:\n",
      "1.0\n",
      "Row means:\n",
      "[[0.0008]\n",
      " [0.0008]\n",
      " [0.0002]\n",
      " ...\n",
      " [0.0002]\n",
      " [0.0008]\n",
      " [0.0004]]\n",
      "Transform to numpy array format:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>schools</th>\n",
       "      <th>sats</th>\n",
       "      <th>sixth</th>\n",
       "      <th>bodies</th>\n",
       "      <th>colleges</th>\n",
       "      <th>mats</th>\n",
       "      <th>responsible</th>\n",
       "      <th>condition</th>\n",
       "      <th>form</th>\n",
       "      <th>improvements</th>\n",
       "      <th>...</th>\n",
       "      <th>establishment</th>\n",
       "      <th>establishing</th>\n",
       "      <th>established</th>\n",
       "      <th>establish</th>\n",
       "      <th>essex</th>\n",
       "      <th>essentials</th>\n",
       "      <th>essential</th>\n",
       "      <th>especially</th>\n",
       "      <th>esol</th>\n",
       "      <th>zumba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.419304</td>\n",
       "      <td>0.262298</td>\n",
       "      <td>0.26131</td>\n",
       "      <td>0.259912</td>\n",
       "      <td>0.259654</td>\n",
       "      <td>0.258748</td>\n",
       "      <td>0.257941</td>\n",
       "      <td>0.251038</td>\n",
       "      <td>0.248952</td>\n",
       "      <td>0.248049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420013</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420014</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420015</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420016</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420017</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420018 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         schools      sats    sixth    bodies  colleges      mats  \\\n",
       "0       0.419304  0.262298  0.26131  0.259912  0.259654  0.258748   \n",
       "1       0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "2       0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "3       0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "4       0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "...          ...       ...      ...       ...       ...       ...   \n",
       "420013  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "420014  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "420015  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "420016  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "420017  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        responsible  condition      form  improvements  ...  establishment  \\\n",
       "0          0.257941   0.251038  0.248952      0.248049  ...            0.0   \n",
       "1          0.000000   0.000000  0.000000      0.000000  ...            0.0   \n",
       "2          0.000000   0.000000  0.000000      0.000000  ...            0.0   \n",
       "3          0.000000   0.000000  0.000000      0.000000  ...            0.0   \n",
       "4          0.000000   0.000000  0.000000      0.000000  ...            0.0   \n",
       "...             ...        ...       ...           ...  ...            ...   \n",
       "420013     0.000000   0.000000  0.000000      0.000000  ...            0.0   \n",
       "420014     0.000000   0.000000  0.000000      0.000000  ...            0.0   \n",
       "420015     0.000000   0.000000  0.000000      0.000000  ...            0.0   \n",
       "420016     0.000000   0.000000  0.000000      0.000000  ...            0.0   \n",
       "420017     0.000000   0.000000  0.000000      0.000000  ...            0.0   \n",
       "\n",
       "        establishing  established  establish  essex  essentials  essential  \\\n",
       "0                0.0          0.0        0.0    0.0         0.0        0.0   \n",
       "1                0.0          0.0        0.0    0.0         0.0        0.0   \n",
       "2                0.0          0.0        0.0    0.0         0.0        0.0   \n",
       "3                0.0          0.0        0.0    0.0         0.0        0.0   \n",
       "4                0.0          0.0        0.0    0.0         0.0        0.0   \n",
       "...              ...          ...        ...    ...         ...        ...   \n",
       "420013           0.0          0.0        0.0    0.0         0.0        0.0   \n",
       "420014           0.0          0.0        0.0    0.0         0.0        0.0   \n",
       "420015           0.0          0.0        0.0    0.0         0.0        0.0   \n",
       "420016           0.0          0.0        0.0    0.0         0.0        0.0   \n",
       "420017           0.0          0.0        0.0    0.0         0.0        0.0   \n",
       "\n",
       "        especially  esol  zumba  \n",
       "0              0.0   0.0    0.0  \n",
       "1              0.0   0.0    0.0  \n",
       "2              0.0   0.0    0.0  \n",
       "3              0.0   0.0    0.0  \n",
       "4              0.0   0.0    0.0  \n",
       "...            ...   ...    ...  \n",
       "420013         0.0   0.0    0.0  \n",
       "420014         0.0   0.0    0.0  \n",
       "420015         0.0   0.0    0.0  \n",
       "420016         0.0   0.0    0.0  \n",
       "420017         0.0   0.0    0.0  \n",
       "\n",
       "[420018 rows x 5000 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tvec = TfidfVectorizer(stop_words='english', ngram_range=(1,1), token_pattern='\\w+', strip_accents='unicode', max_features=5000)\n",
    "tvec.fit(corpus_train)\n",
    "vecmat = tvec.transform(corpus_train)\n",
    "\n",
    "print(\"Number of nonzero entries:\")\n",
    "print(vecmat.nnz)\n",
    "print(\"Highest count:\")\n",
    "print(vecmat.max())\n",
    "print(\"Row means:\")\n",
    "print(vecmat.mean(axis=1))\n",
    "print(\"Transform to numpy array format:\")\n",
    "print(vecmat.toarray())\n",
    "\n",
    "df = pd.DataFrame(tvec.transform(corpus_train).toarray(),\n",
    "                  columns=tvec.get_feature_names())\n",
    "df = df.transpose().sort_values(0, ascending=False).transpose()\n",
    "#df['target'] = data_train.target\n",
    "df_train = df\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Stemming and lemmatizing words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up some lemmatising and stemming functions to pass as the tokeniser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:50:08.975713Z",
     "iopub.status.busy": "2021-06-03T05:50:08.975327Z",
     "iopub.status.idle": "2021-06-03T05:50:09.090950Z",
     "shell.execute_reply": "2021-06-03T05:50:09.089796Z",
     "shell.execute_reply.started": "2021-06-03T05:50:08.975658Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text: str) -> str:\n",
    "    \"\"\"Lemmatises str\"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    return ps.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text: str) -> List[str]:\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:50:09.094077Z",
     "iopub.status.busy": "2021-06-03T05:50:09.093870Z",
     "iopub.status.idle": "2021-06-03T05:50:10.631675Z",
     "shell.execute_reply": "2021-06-03T05:50:10.630995Z",
     "shell.execute_reply.started": "2021-06-03T05:50:09.094055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['A', 'Second', 'World', 'War', 'veteran', 'from', 'Cambridgeshire', 'has', 'been', 'awarded', 'a', 'grant', 'towards', 'travel', 'costs', 'for', 'a', 'commemorative', 'visit', 'to', 'Normandy', 'in', 'June', '2010.', 'Funding', 'has', 'been', 'provided', 'for', 'one', 'Second', 'World', 'War', 'veteran', 'and', 'one', 'carer.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['second', 'world', 'veteran', 'cambridgeshir', 'award', 'grant', 'travel', 'cost', 'commemor', 'visit', 'normandi', 'june', 'fund', 'provid', 'second', 'world', 'veteran', 'carer']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = corpus_train.iloc[10]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))\n",
    "assert type(doc_sample) == str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a working tokenizer, we should investigate a tfidf vectoriser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:50:10.633271Z",
     "iopub.status.busy": "2021-06-03T05:50:10.633070Z",
     "iopub.status.idle": "2021-06-03T05:50:10.908156Z",
     "shell.execute_reply": "2021-06-03T05:50:10.907476Z",
     "shell.execute_reply.started": "2021-06-03T05:50:10.633250Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = data['description'].sample(10000) # sample first as this could take a long time\n",
    "corpus_train, corpus_test = train_test_split(corpus, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:50:10.909760Z",
     "iopub.status.busy": "2021-06-03T05:50:10.909587Z",
     "iopub.status.idle": "2021-06-03T05:50:33.413174Z",
     "shell.execute_reply": "2021-06-03T05:50:33.412504Z",
     "shell.execute_reply.started": "2021-06-03T05:50:10.909741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(ngram_range=(1, 2), stop_words='english',\n",
       "                strip_accents='unicode', token_pattern='\\\\w+',\n",
       "                tokenizer=<function preprocess at 0x7fbd6c486af0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flintshir base</th>\n",
       "      <th>enabl flintshir</th>\n",
       "      <th>plan support</th>\n",
       "      <th>inform plan</th>\n",
       "      <th>undertak comprehens</th>\n",
       "      <th>effici review</th>\n",
       "      <th>comprehens effici</th>\n",
       "      <th>chariti undertak</th>\n",
       "      <th>salari inform</th>\n",
       "      <th>offic enabl</th>\n",
       "      <th>...</th>\n",
       "      <th>express fundament</th>\n",
       "      <th>express excitatori</th>\n",
       "      <th>express develop</th>\n",
       "      <th>express enter</th>\n",
       "      <th>express enjoy</th>\n",
       "      <th>express embryon</th>\n",
       "      <th>express effect</th>\n",
       "      <th>express diseas</th>\n",
       "      <th>express differ</th>\n",
       "      <th>zumo children</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.251263</td>\n",
       "      <td>0.251263</td>\n",
       "      <td>0.251263</td>\n",
       "      <td>0.251263</td>\n",
       "      <td>0.251263</td>\n",
       "      <td>0.251263</td>\n",
       "      <td>0.251263</td>\n",
       "      <td>0.240302</td>\n",
       "      <td>0.240302</td>\n",
       "      <td>0.232525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 76564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      flintshir base  enabl flintshir  plan support  inform plan  \\\n",
       "0           0.251263         0.251263      0.251263     0.251263   \n",
       "1           0.000000         0.000000      0.000000     0.000000   \n",
       "2           0.000000         0.000000      0.000000     0.000000   \n",
       "3           0.000000         0.000000      0.000000     0.000000   \n",
       "4           0.000000         0.000000      0.000000     0.000000   \n",
       "...              ...              ...           ...          ...   \n",
       "7995        0.000000         0.000000      0.000000     0.000000   \n",
       "7996        0.000000         0.000000      0.000000     0.000000   \n",
       "7997        0.000000         0.000000      0.000000     0.000000   \n",
       "7998        0.000000         0.000000      0.000000     0.000000   \n",
       "7999        0.000000         0.000000      0.000000     0.000000   \n",
       "\n",
       "      undertak comprehens  effici review  comprehens effici  chariti undertak  \\\n",
       "0                0.251263       0.251263           0.251263          0.240302   \n",
       "1                0.000000       0.000000           0.000000          0.000000   \n",
       "2                0.000000       0.000000           0.000000          0.000000   \n",
       "3                0.000000       0.000000           0.000000          0.000000   \n",
       "4                0.000000       0.000000           0.000000          0.000000   \n",
       "...                   ...            ...                ...               ...   \n",
       "7995             0.000000       0.000000           0.000000          0.000000   \n",
       "7996             0.000000       0.000000           0.000000          0.000000   \n",
       "7997             0.000000       0.000000           0.000000          0.000000   \n",
       "7998             0.000000       0.000000           0.000000          0.000000   \n",
       "7999             0.000000       0.000000           0.000000          0.000000   \n",
       "\n",
       "      salari inform  offic enabl  ...  express fundament  express excitatori  \\\n",
       "0          0.240302     0.232525  ...                0.0                 0.0   \n",
       "1          0.000000     0.000000  ...                0.0                 0.0   \n",
       "2          0.000000     0.000000  ...                0.0                 0.0   \n",
       "3          0.000000     0.000000  ...                0.0                 0.0   \n",
       "4          0.000000     0.000000  ...                0.0                 0.0   \n",
       "...             ...          ...  ...                ...                 ...   \n",
       "7995       0.000000     0.000000  ...                0.0                 0.0   \n",
       "7996       0.000000     0.000000  ...                0.0                 0.0   \n",
       "7997       0.000000     0.000000  ...                0.0                 0.0   \n",
       "7998       0.000000     0.000000  ...                0.0                 0.0   \n",
       "7999       0.000000     0.000000  ...                0.0                 0.0   \n",
       "\n",
       "      express develop  express enter  express enjoy  express embryon  \\\n",
       "0                 0.0            0.0            0.0              0.0   \n",
       "1                 0.0            0.0            0.0              0.0   \n",
       "2                 0.0            0.0            0.0              0.0   \n",
       "3                 0.0            0.0            0.0              0.0   \n",
       "4                 0.0            0.0            0.0              0.0   \n",
       "...               ...            ...            ...              ...   \n",
       "7995              0.0            0.0            0.0              0.0   \n",
       "7996              0.0            0.0            0.0              0.0   \n",
       "7997              0.0            0.0            0.0              0.0   \n",
       "7998              0.0            0.0            0.0              0.0   \n",
       "7999              0.0            0.0            0.0              0.0   \n",
       "\n",
       "      express effect  express diseas  express differ  zumo children  \n",
       "0                0.0             0.0             0.0            0.0  \n",
       "1                0.0             0.0             0.0            0.0  \n",
       "2                0.0             0.0             0.0            0.0  \n",
       "3                0.0             0.0             0.0            0.0  \n",
       "4                0.0             0.0             0.0            0.0  \n",
       "...              ...             ...             ...            ...  \n",
       "7995             0.0             0.0             0.0            0.0  \n",
       "7996             0.0             0.0             0.0            0.0  \n",
       "7997             0.0             0.0             0.0            0.0  \n",
       "7998             0.0             0.0             0.0            0.0  \n",
       "7999             0.0             0.0             0.0            0.0  \n",
       "\n",
       "[8000 rows x 76564 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tvec = TfidfVectorizer(stop_words='english', ngram_range=(1,2), token_pattern='\\w+', strip_accents='unicode', tokenizer=preprocess)\n",
    "tvec.fit(corpus_train)\n",
    "vecmat = tvec.transform(corpus_train)\n",
    "\n",
    "df = pd.DataFrame(tvec.transform(corpus_train).toarray(),\n",
    "                  columns=tvec.get_feature_names())\n",
    "df = df.transpose().sort_values(0, ascending=False).transpose()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:50:33.414658Z",
     "iopub.status.busy": "2021-06-03T05:50:33.414324Z",
     "iopub.status.idle": "2021-06-03T05:50:45.085152Z",
     "shell.execute_reply": "2021-06-03T05:50:45.084127Z",
     "shell.execute_reply.started": "2021-06-03T05:50:33.414636Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc['Mean'] = df.mean()\n",
    "df.loc['Sum'] = df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check words with the highest mean values to see how the ordered and lem/stem words look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:50:45.086931Z",
     "iopub.status.busy": "2021-06-03T05:50:45.086719Z",
     "iopub.status.idle": "2021-06-03T05:50:45.452594Z",
     "shell.execute_reply": "2021-06-03T05:50:45.451918Z",
     "shell.execute_reply.started": "2021-06-03T05:50:45.086909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train',\n",
       " 'travel',\n",
       " 'titl',\n",
       " 'travel cost',\n",
       " 'teacher',\n",
       " 'titl covid',\n",
       " 'travel veteran',\n",
       " 'time',\n",
       " 'traine',\n",
       " 'trip']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [c for c in df.columns if c.startswith('t')]\n",
    "words_to_check = df[columns].T.sort_values('Mean', ascending=False)\n",
    "list(words_to_check.index)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:50:45.454091Z",
     "iopub.status.busy": "2021-06-03T05:50:45.453842Z",
     "iopub.status.idle": "2021-06-03T05:50:45.509764Z",
     "shell.execute_reply": "2021-06-03T05:50:45.509187Z",
     "shell.execute_reply.started": "2021-06-03T05:50:45.454066Z"
    }
   },
   "outputs": [],
   "source": [
    "#df[word_to_analyse].iloc[:-1].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:50:45.510896Z",
     "iopub.status.busy": "2021-06-03T05:50:45.510736Z",
     "iopub.status.idle": "2021-06-03T05:50:45.580486Z",
     "shell.execute_reply": "2021-06-03T05:50:45.579879Z",
     "shell.execute_reply.started": "2021-06-03T05:50:45.510878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index for 'train' = 418\n",
      "\n",
      "ORIGINAL:\n",
      "Training Grant (including Trader Training)\n",
      "\n",
      "ORDERED LEM/STEM:\n",
      "{'train': 0.42, 'trader train': 0.42, 'includ trader': 0.42, 'trader': 0.42, 'train grant': 0.41, 'grant includ': 0.26, 'includ': 0.2, 'grant': 0.15}\n"
     ]
    }
   ],
   "source": [
    "word_to_analyse = 'train'\n",
    "idx = df[word_to_analyse].iloc[:-1].idxmax()\n",
    "print(f\"index for '{word_to_analyse}' = {idx}\")\n",
    "print()\n",
    "print(f\"ORIGINAL:\\n{corpus_train.iloc[idx]}\")\n",
    "print()\n",
    "ordered_words = df.iloc[idx][df.iloc[idx]>0].sort_values(ascending=False)\n",
    "#print(f\"ORDERED LEM/STEM:\\n{' '.join(list(ordered_words.index))}\")\n",
    "#print()\n",
    "scores = {i: round(v, 2) for i, v in zip(list(ordered_words.index), list(ordered_words.values))}\n",
    "print(f\"ORDERED LEM/STEM:\\n{scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some high-scoring groups of words in this example that include probably redundant words (such as 'includ' and 'grant' in 'train grant') and could be replicated elsewhere.  We could try reducing to just 1 n-gram or we could reduce to just nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 1-grams (nouns and superlatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:50:45.582043Z",
     "iopub.status.busy": "2021-06-03T05:50:45.581667Z",
     "iopub.status.idle": "2021-06-03T05:50:45.657750Z",
     "shell.execute_reply": "2021-06-03T05:50:45.656916Z",
     "shell.execute_reply.started": "2021-06-03T05:50:45.581995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55783</th>\n",
       "      <td>Dengue shock syndrome (DSS) is the commonest l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61818</th>\n",
       "      <td>Dance House is involved in a range of outreach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179624</th>\n",
       "      <td>This school will create a courtyard sensory ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164973</th>\n",
       "      <td>This group  promote awareness of interest in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346490</th>\n",
       "      <td>As requested by the donor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126172</th>\n",
       "      <td>A Second World War Veteran from Somerset has b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60538</th>\n",
       "      <td>Funding is required to install new CCTV camera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27494</th>\n",
       "      <td>Funding under Sport England's COVID-19 Communi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256247</th>\n",
       "      <td>Presteigne &amp; Norton Chamber of Trade, Commerce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260931</th>\n",
       "      <td>This project aims to reduce social isolation, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              description\n",
       "55783   Dengue shock syndrome (DSS) is the commonest l...\n",
       "61818   Dance House is involved in a range of outreach...\n",
       "179624  This school will create a courtyard sensory ga...\n",
       "164973  This group  promote awareness of interest in a...\n",
       "346490                          As requested by the donor\n",
       "...                                                   ...\n",
       "126172  A Second World War Veteran from Somerset has b...\n",
       "60538   Funding is required to install new CCTV camera...\n",
       "27494   Funding under Sport England's COVID-19 Communi...\n",
       "256247  Presteigne & Norton Chamber of Trade, Commerce...\n",
       "260931  This project aims to reduce social isolation, ...\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " pd.DataFrame(data['description'].sample(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T05:50:45.661244Z",
     "iopub.status.busy": "2021-06-03T05:50:45.661060Z",
     "iopub.status.idle": "2021-06-03T06:04:11.523379Z",
     "shell.execute_reply": "2021-06-03T06:04:11.522577Z",
     "shell.execute_reply.started": "2021-06-03T05:50:45.661225Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 525023/525023 [13:25<00:00, 651.65it/s] \n"
     ]
    }
   ],
   "source": [
    "corpus2 = pd.DataFrame(data['description'])\n",
    "tqdm.pandas()\n",
    "corpus2['string_nouns'] = corpus2['description'].progress_apply(lambda x: ' '.join(create_document_grammars(x, grammar=['NN','RBS','JJS'])))\n",
    "corpus2 = corpus2['string_nouns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T06:04:11.525168Z",
     "iopub.status.busy": "2021-06-03T06:04:11.524963Z",
     "iopub.status.idle": "2021-06-03T06:04:11.728458Z",
     "shell.execute_reply": "2021-06-03T06:04:11.727813Z",
     "shell.execute_reply.started": "2021-06-03T06:04:11.525146Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus2_train, corpus2_test = train_test_split(corpus2, train_size=0.8, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T06:04:11.729869Z",
     "iopub.status.busy": "2021-06-03T06:04:11.729505Z",
     "iopub.status.idle": "2021-06-03T06:11:53.281916Z",
     "shell.execute_reply": "2021-06-03T06:11:53.280638Z",
     "shell.execute_reply.started": "2021-06-03T06:04:11.729849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english',\n",
       "                strip_accents='unicode', token_pattern='\\\\w+',\n",
       "                tokenizer=<function preprocess at 0x7fbd6c486af0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instal vehicl</th>\n",
       "      <th>individu point</th>\n",
       "      <th>vehicl individu</th>\n",
       "      <th>point charg</th>\n",
       "      <th>charg</th>\n",
       "      <th>vehicl</th>\n",
       "      <th>point</th>\n",
       "      <th>instal</th>\n",
       "      <th>individu</th>\n",
       "      <th>plan</th>\n",
       "      <th>...</th>\n",
       "      <th>faith</th>\n",
       "      <th>fairer</th>\n",
       "      <th>fair</th>\n",
       "      <th>failur</th>\n",
       "      <th>factor</th>\n",
       "      <th>fact</th>\n",
       "      <th>facilit</th>\n",
       "      <th>facil villag</th>\n",
       "      <th>facil train</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.368381</td>\n",
       "      <td>0.368381</td>\n",
       "      <td>0.368289</td>\n",
       "      <td>0.368011</td>\n",
       "      <td>0.338802</td>\n",
       "      <td>0.310885</td>\n",
       "      <td>0.30418</td>\n",
       "      <td>0.287229</td>\n",
       "      <td>0.26657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420013</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420014</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420015</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420016</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420017</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420018 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        instal vehicl  individu point  vehicl individu  point charg     charg  \\\n",
       "0            0.368381        0.368381         0.368289     0.368011  0.338802   \n",
       "1            0.000000        0.000000         0.000000     0.000000  0.000000   \n",
       "2            0.000000        0.000000         0.000000     0.000000  0.000000   \n",
       "3            0.000000        0.000000         0.000000     0.000000  0.000000   \n",
       "4            0.000000        0.000000         0.000000     0.000000  0.000000   \n",
       "...               ...             ...              ...          ...       ...   \n",
       "420013       0.000000        0.000000         0.000000     0.000000  0.000000   \n",
       "420014       0.000000        0.000000         0.000000     0.000000  0.000000   \n",
       "420015       0.000000        0.000000         0.000000     0.000000  0.000000   \n",
       "420016       0.000000        0.000000         0.000000     0.000000  0.000000   \n",
       "420017       0.000000        0.000000         0.000000     0.000000  0.000000   \n",
       "\n",
       "          vehicl    point    instal  individu  plan  ...  faith  fairer  fair  \\\n",
       "0       0.310885  0.30418  0.287229   0.26657   0.0  ...    0.0     0.0   0.0   \n",
       "1       0.000000  0.00000  0.000000   0.00000   0.0  ...    0.0     0.0   0.0   \n",
       "2       0.000000  0.00000  0.000000   0.00000   0.0  ...    0.0     0.0   0.0   \n",
       "3       0.000000  0.00000  0.000000   0.00000   0.0  ...    0.0     0.0   0.0   \n",
       "4       0.000000  0.00000  0.000000   0.00000   0.0  ...    0.0     0.0   0.0   \n",
       "...          ...      ...       ...       ...   ...  ...    ...     ...   ...   \n",
       "420013  0.000000  0.00000  0.000000   0.00000   0.0  ...    0.0     0.0   0.0   \n",
       "420014  0.000000  0.00000  0.000000   0.00000   0.0  ...    0.0     0.0   0.0   \n",
       "420015  0.000000  0.00000  0.000000   0.00000   0.0  ...    0.0     0.0   0.0   \n",
       "420016  0.000000  0.00000  0.000000   0.00000   0.0  ...    0.0     0.0   0.0   \n",
       "420017  0.000000  0.00000  0.000000   0.00000   0.0  ...    0.0     0.0   0.0   \n",
       "\n",
       "        failur  factor  fact  facilit  facil villag  facil train  zoom  \n",
       "0          0.0     0.0   0.0      0.0           0.0          0.0   0.0  \n",
       "1          0.0     0.0   0.0      0.0           0.0          0.0   0.0  \n",
       "2          0.0     0.0   0.0      0.0           0.0          0.0   0.0  \n",
       "3          0.0     0.0   0.0      0.0           0.0          0.0   0.0  \n",
       "4          0.0     0.0   0.0      0.0           0.0          0.0   0.0  \n",
       "...        ...     ...   ...      ...           ...          ...   ...  \n",
       "420013     0.0     0.0   0.0      0.0           0.0          0.0   0.0  \n",
       "420014     0.0     0.0   0.0      0.0           0.0          0.0   0.0  \n",
       "420015     0.0     0.0   0.0      0.0           0.0          0.0   0.0  \n",
       "420016     0.0     0.0   0.0      0.0           0.0          0.0   0.0  \n",
       "420017     0.0     0.0   0.0      0.0           0.0          0.0   0.0  \n",
       "\n",
       "[420018 rows x 5000 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tvec = TfidfVectorizer(stop_words='english', ngram_range=(1,2), token_pattern='\\w+', strip_accents='unicode', tokenizer=preprocess, max_features=5000)\n",
    "tvec.fit(corpus2_train)\n",
    "vecmat = tvec.transform(corpus2_train)\n",
    "\n",
    "df_nouns = pd.DataFrame(tvec.transform(corpus2_train).toarray(),\n",
    "                  columns=tvec.get_feature_names())\n",
    "df_nouns = df_nouns.transpose().sort_values(0, ascending=False).transpose()\n",
    "display(df_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T06:11:53.284999Z",
     "iopub.status.busy": "2021-06-03T06:11:53.284734Z",
     "iopub.status.idle": "2021-06-03T06:13:26.208008Z",
     "shell.execute_reply": "2021-06-03T06:13:26.205791Z",
     "shell.execute_reply.started": "2021-06-03T06:11:53.284961Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nouns.loc['Mean'] = df_nouns.mean()\n",
    "df_nouns.loc['Sum'] = df_nouns.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check words with the highest mean values to see how the ordered and lem/stem words look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T06:13:26.211262Z",
     "iopub.status.busy": "2021-06-03T06:13:26.210997Z",
     "iopub.status.idle": "2021-06-03T06:13:28.007545Z",
     "shell.execute_reply": "2021-06-03T06:13:28.006865Z",
     "shell.execute_reply.started": "2021-06-03T06:13:26.211229Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train',\n",
       " 'teacher',\n",
       " 'trip',\n",
       " 'train apprenticeship',\n",
       " 'time',\n",
       " 'trust',\n",
       " 'transport',\n",
       " 'traine',\n",
       " 'team',\n",
       " 'travel']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [c for c in df_nouns.columns if c.startswith('t')]\n",
    "words_to_check = df_nouns[columns].T.sort_values('Mean', ascending=False)\n",
    "list(words_to_check.index)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T14:16:34.493629Z",
     "iopub.status.busy": "2021-06-02T14:16:34.493387Z",
     "iopub.status.idle": "2021-06-02T14:16:34.560663Z",
     "shell.execute_reply": "2021-06-02T14:16:34.559827Z",
     "shell.execute_reply.started": "2021-06-02T14:16:34.493604Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_nouns[word_to_analyse].iloc[:-1].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T06:21:44.331445Z",
     "iopub.status.busy": "2021-06-03T06:21:44.331235Z",
     "iopub.status.idle": "2021-06-03T06:21:44.506421Z",
     "shell.execute_reply": "2021-06-03T06:21:44.505345Z",
     "shell.execute_reply.started": "2021-06-03T06:21:44.331425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index for 'trip' = 179838\n",
      "\n",
      "ORIGINAL:\n",
      "public families methods day sufferers support grant effects conference awareness group trips\n",
      "\n",
      "ORDERED LEM/STEM:\n",
      "{'suffer': 0.36, 'awar group': 0.36, 'group trip': 0.33, 'confer': 0.31, 'method': 0.31, 'support grant': 0.3, 'effect': 0.28, 'public': 0.26, 'trip': 0.24, 'awar': 0.24, 'famili': 0.19, 'support': 0.16, 'group': 0.13, 'grant': 0.12}\n"
     ]
    }
   ],
   "source": [
    "word_to_analyse = 'trip'\n",
    "idx_df = df_nouns[word_to_analyse].iloc[:-1].sort_values(ascending=False)\n",
    "idx = idx_df[idx_df>0].sample(1).index[0]\n",
    "print(f\"index for '{word_to_analyse}' = {idx}\")\n",
    "print()\n",
    "print(f\"ORIGINAL:\\n{corpus2_train.iloc[idx]}\")\n",
    "print()\n",
    "ordered_words = df_nouns.iloc[idx][df_nouns.iloc[idx]>0].sort_values(ascending=False)\n",
    "#print(f\"ORDERED LEM/STEM:\\n{' '.join(list(ordered_words.index))}\")\n",
    "#print()\n",
    "scores = {i: round(v, 2) for i, v in zip(list(ordered_words.index), list(ordered_words.values))}\n",
    "print(f\"ORDERED LEM/STEM:\\n{scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'parent children' has a higher score than parent and children put together; but how do we account for grouping of words when a user types in an engine?  Do we engineer it in a way that allows for predictions if 'children parent' is typed in.  What about 'parent child'?  There is also the added problem of these nouns appearing together artidificially (there'll be other PS types between nouns, and nouns could move about a sentence.  Perhaps it's better to stick with individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T07:04:48.313425Z",
     "iopub.status.busy": "2021-06-03T07:04:48.313220Z",
     "iopub.status.idle": "2021-06-03T07:12:24.459126Z",
     "shell.execute_reply": "2021-06-03T07:12:24.457757Z",
     "shell.execute_reply.started": "2021-06-03T07:04:48.313406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=5000, stop_words='english',\n",
       "                strip_accents='unicode', token_pattern='\\\\w+',\n",
       "                tokenizer=<function preprocess at 0x7fbd6c486af0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>charg</th>\n",
       "      <th>vehicl</th>\n",
       "      <th>point</th>\n",
       "      <th>instal</th>\n",
       "      <th>individu</th>\n",
       "      <th>phosphoryl</th>\n",
       "      <th>photoshop</th>\n",
       "      <th>photoreceptor</th>\n",
       "      <th>photographi</th>\n",
       "      <th>photograph</th>\n",
       "      <th>...</th>\n",
       "      <th>falmouth</th>\n",
       "      <th>fall</th>\n",
       "      <th>falkirk</th>\n",
       "      <th>falciparum</th>\n",
       "      <th>faith</th>\n",
       "      <th>fairer</th>\n",
       "      <th>fair</th>\n",
       "      <th>failur</th>\n",
       "      <th>fail</th>\n",
       "      <th>zumba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500887</td>\n",
       "      <td>0.459614</td>\n",
       "      <td>0.449702</td>\n",
       "      <td>0.424641</td>\n",
       "      <td>0.3941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420013</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420014</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420015</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420016</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420017</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420018 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           charg    vehicl     point    instal  individu  phosphoryl  \\\n",
       "0       0.500887  0.459614  0.449702  0.424641    0.3941         0.0   \n",
       "1       0.000000  0.000000  0.000000  0.000000    0.0000         0.0   \n",
       "2       0.000000  0.000000  0.000000  0.000000    0.0000         0.0   \n",
       "3       0.000000  0.000000  0.000000  0.000000    0.0000         0.0   \n",
       "4       0.000000  0.000000  0.000000  0.000000    0.0000         0.0   \n",
       "...          ...       ...       ...       ...       ...         ...   \n",
       "420013  0.000000  0.000000  0.000000  0.000000    0.0000         0.0   \n",
       "420014  0.000000  0.000000  0.000000  0.000000    0.0000         0.0   \n",
       "420015  0.000000  0.000000  0.000000  0.000000    0.0000         0.0   \n",
       "420016  0.000000  0.000000  0.000000  0.000000    0.0000         0.0   \n",
       "420017  0.000000  0.000000  0.000000  0.000000    0.0000         0.0   \n",
       "\n",
       "        photoshop  photoreceptor  photographi  photograph  ...  falmouth  \\\n",
       "0             0.0            0.0          0.0         0.0  ...       0.0   \n",
       "1             0.0            0.0          0.0         0.0  ...       0.0   \n",
       "2             0.0            0.0          0.0         0.0  ...       0.0   \n",
       "3             0.0            0.0          0.0         0.0  ...       0.0   \n",
       "4             0.0            0.0          0.0         0.0  ...       0.0   \n",
       "...           ...            ...          ...         ...  ...       ...   \n",
       "420013        0.0            0.0          0.0         0.0  ...       0.0   \n",
       "420014        0.0            0.0          0.0         0.0  ...       0.0   \n",
       "420015        0.0            0.0          0.0         0.0  ...       0.0   \n",
       "420016        0.0            0.0          0.0         0.0  ...       0.0   \n",
       "420017        0.0            0.0          0.0         0.0  ...       0.0   \n",
       "\n",
       "        fall  falkirk  falciparum  faith  fairer  fair  failur  fail  zumba  \n",
       "0        0.0      0.0         0.0    0.0     0.0   0.0     0.0   0.0    0.0  \n",
       "1        0.0      0.0         0.0    0.0     0.0   0.0     0.0   0.0    0.0  \n",
       "2        0.0      0.0         0.0    0.0     0.0   0.0     0.0   0.0    0.0  \n",
       "3        0.0      0.0         0.0    0.0     0.0   0.0     0.0   0.0    0.0  \n",
       "4        0.0      0.0         0.0    0.0     0.0   0.0     0.0   0.0    0.0  \n",
       "...      ...      ...         ...    ...     ...   ...     ...   ...    ...  \n",
       "420013   0.0      0.0         0.0    0.0     0.0   0.0     0.0   0.0    0.0  \n",
       "420014   0.0      0.0         0.0    0.0     0.0   0.0     0.0   0.0    0.0  \n",
       "420015   0.0      0.0         0.0    0.0     0.0   0.0     0.0   0.0    0.0  \n",
       "420016   0.0      0.0         0.0    0.0     0.0   0.0     0.0   0.0    0.0  \n",
       "420017   0.0      0.0         0.0    0.0     0.0   0.0     0.0   0.0    0.0  \n",
       "\n",
       "[420018 rows x 5000 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tvec = TfidfVectorizer(stop_words='english', ngram_range=(1,1), token_pattern='\\w+', strip_accents='unicode', tokenizer=preprocess, max_features=5000)\n",
    "tvec.fit(corpus2_train)\n",
    "vecmat = tvec.transform(corpus2_train)\n",
    "\n",
    "df_nouns_1n = pd.DataFrame(tvec.transform(corpus2_train).toarray(),\n",
    "                  columns=tvec.get_feature_names())\n",
    "df_nouns_1n = df_nouns_1n.transpose().sort_values(0, ascending=False).transpose()\n",
    "display(df_nouns_1n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T07:12:24.462892Z",
     "iopub.status.busy": "2021-06-03T07:12:24.462622Z",
     "iopub.status.idle": "2021-06-03T07:14:00.678264Z",
     "shell.execute_reply": "2021-06-03T07:14:00.676373Z",
     "shell.execute_reply.started": "2021-06-03T07:12:24.462855Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nouns_1n.loc['Mean'] = df_nouns_1n.mean()\n",
    "df_nouns_1n.loc['Sum'] = df_nouns_1n.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T07:14:00.685318Z",
     "iopub.status.busy": "2021-06-03T07:14:00.684973Z",
     "iopub.status.idle": "2021-06-03T07:14:02.928026Z",
     "shell.execute_reply": "2021-06-03T07:14:02.927416Z",
     "shell.execute_reply.started": "2021-06-03T07:14:00.685284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['england',\n",
       " 'equip',\n",
       " 'event',\n",
       " 'emerg',\n",
       " 'educ',\n",
       " 'employ',\n",
       " 'environ',\n",
       " 'experi',\n",
       " 'exercis',\n",
       " 'engag']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [c for c in df_nouns_1n.columns if c.startswith('e')]\n",
    "words_to_check = df_nouns_1n[columns].T.sort_values('Mean', ascending=False)\n",
    "list(words_to_check.index)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T07:18:15.642476Z",
     "iopub.status.busy": "2021-06-03T07:18:15.642275Z",
     "iopub.status.idle": "2021-06-03T07:18:15.759802Z",
     "shell.execute_reply": "2021-06-03T07:18:15.759182Z",
     "shell.execute_reply.started": "2021-06-03T07:18:15.642457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index for 'environ' = 228302\n",
      "\n",
      "ORIGINAL:\n",
      "area community shrubs pond school project boards environment grounds wider manchester funding children information trees organisation access\n",
      "\n",
      "ORDERED LEM/STEM:\n",
      "{'shrub': 0.4, 'pond': 0.35, 'tree': 0.32, 'board': 0.31, 'manchest': 0.31, 'ground': 0.29, 'wider': 0.25, 'inform': 0.23, 'environ': 0.22, 'access': 0.2, 'organis': 0.17, 'area': 0.17, 'children': 0.16, 'school': 0.15, 'commun': 0.12, 'fund': 0.11, 'project': 0.1}\n"
     ]
    }
   ],
   "source": [
    "word_to_analyse = 'environ'\n",
    "idx_df = df_nouns_1n[word_to_analyse].iloc[:-1].sort_values(ascending=False)\n",
    "idx = idx_df[idx_df>0].sample(1).index[0]\n",
    "print(f\"index for '{word_to_analyse}' = {idx}\")\n",
    "print()\n",
    "print(f\"ORIGINAL:\\n{corpus2_train.iloc[idx]}\")\n",
    "print()\n",
    "ordered_words = df_nouns_1n.iloc[idx][df_nouns_1n.iloc[idx]>0].sort_values(ascending=False)\n",
    "scores = {i: round(v, 2) for i, v in zip(list(ordered_words.index), list(ordered_words.values))}\n",
    "print(f\"ORDERED LEM/STEM:\\n{scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking better.  I can see how a user might type these words into a keyword search bar and how a machine learning model (with limited computational resources) might handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T07:14:03.093202Z",
     "iopub.status.busy": "2021-06-03T07:14:03.092971Z",
     "iopub.status.idle": "2021-06-03T07:14:45.711977Z",
     "shell.execute_reply": "2021-06-03T07:14:45.710754Z",
     "shell.execute_reply.started": "2021-06-03T07:14:03.093184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chicken</th>\n",
       "      <th>climber</th>\n",
       "      <th>hotel</th>\n",
       "      <th>signag</th>\n",
       "      <th>classroom</th>\n",
       "      <th>plant</th>\n",
       "      <th>day</th>\n",
       "      <th>materi</th>\n",
       "      <th>hous</th>\n",
       "      <th>educ</th>\n",
       "      <th>...</th>\n",
       "      <th>fareshar</th>\n",
       "      <th>fareham</th>\n",
       "      <th>fare</th>\n",
       "      <th>fan</th>\n",
       "      <th>famili</th>\n",
       "      <th>falmouth</th>\n",
       "      <th>fall</th>\n",
       "      <th>falkirk</th>\n",
       "      <th>falciparum</th>\n",
       "      <th>zumba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.401735</td>\n",
       "      <td>0.401735</td>\n",
       "      <td>0.355323</td>\n",
       "      <td>0.332787</td>\n",
       "      <td>0.288056</td>\n",
       "      <td>0.258669</td>\n",
       "      <td>0.250242</td>\n",
       "      <td>0.211972</td>\n",
       "      <td>0.20272</td>\n",
       "      <td>0.187869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105000</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105001</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105002</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105003</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105004</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105005 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         chicken   climber     hotel    signag  classroom     plant       day  \\\n",
       "0       0.401735  0.401735  0.355323  0.332787   0.288056  0.258669  0.250242   \n",
       "1       0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "2       0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "3       0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "4       0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "...          ...       ...       ...       ...        ...       ...       ...   \n",
       "105000  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "105001  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "105002  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "105003  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "105004  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "\n",
       "          materi     hous      educ  ...  fareshar  fareham  fare  fan  \\\n",
       "0       0.211972  0.20272  0.187869  ...       0.0      0.0   0.0  0.0   \n",
       "1       0.000000  0.00000  0.000000  ...       0.0      0.0   0.0  0.0   \n",
       "2       0.000000  0.00000  0.000000  ...       0.0      0.0   0.0  0.0   \n",
       "3       0.000000  0.00000  0.000000  ...       0.0      0.0   0.0  0.0   \n",
       "4       0.000000  0.00000  0.000000  ...       0.0      0.0   0.0  0.0   \n",
       "...          ...      ...       ...  ...       ...      ...   ...  ...   \n",
       "105000  0.000000  0.00000  0.000000  ...       0.0      0.0   0.0  0.0   \n",
       "105001  0.000000  0.00000  0.000000  ...       0.0      0.0   0.0  0.0   \n",
       "105002  0.000000  0.00000  0.000000  ...       0.0      0.0   0.0  0.0   \n",
       "105003  0.000000  0.00000  0.000000  ...       0.0      0.0   0.0  0.0   \n",
       "105004  0.000000  0.00000  0.000000  ...       0.0      0.0   0.0  0.0   \n",
       "\n",
       "        famili  falmouth  fall  falkirk  falciparum  zumba  \n",
       "0          0.0       0.0   0.0      0.0         0.0    0.0  \n",
       "1          0.0       0.0   0.0      0.0         0.0    0.0  \n",
       "2          0.0       0.0   0.0      0.0         0.0    0.0  \n",
       "3          0.0       0.0   0.0      0.0         0.0    0.0  \n",
       "4          0.0       0.0   0.0      0.0         0.0    0.0  \n",
       "...        ...       ...   ...      ...         ...    ...  \n",
       "105000     0.0       0.0   0.0      0.0         0.0    0.0  \n",
       "105001     0.0       0.0   0.0      0.0         0.0    0.0  \n",
       "105002     0.0       0.0   0.0      0.0         0.0    0.0  \n",
       "105003     0.0       0.0   0.0      0.0         0.0    0.0  \n",
       "105004     0.0       0.0   0.0      0.0         0.0    0.0  \n",
       "\n",
       "[105005 rows x 5000 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_nouns_1n_test = pd.DataFrame(tvec.transform(corpus2_test).toarray(),\n",
    "                  columns=tvec.get_feature_names())\n",
    "df_nouns_1n_test = df_nouns_1n_test.transpose().sort_values(0, ascending=False).transpose()\n",
    "display(df_nouns_1n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T07:19:42.738511Z",
     "iopub.status.busy": "2021-06-03T07:19:42.738247Z",
     "iopub.status.idle": "2021-06-03T07:19:53.682194Z",
     "shell.execute_reply": "2021-06-03T07:19:53.680188Z",
     "shell.execute_reply.started": "2021-06-03T07:19:42.738485Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nouns_1n_test.loc['Mean'] = df_nouns_1n_test.mean()\n",
    "df_nouns_1n_test.loc['Sum'] = df_nouns_1n_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T07:19:55.327156Z",
     "iopub.status.busy": "2021-06-03T07:19:55.326851Z",
     "iopub.status.idle": "2021-06-03T07:19:55.990950Z",
     "shell.execute_reply": "2021-06-03T07:19:55.990227Z",
     "shell.execute_reply.started": "2021-06-03T07:19:55.327128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['commun',\n",
       " 'cost',\n",
       " 'children',\n",
       " 'capit',\n",
       " 'club',\n",
       " 'carer',\n",
       " 'condit',\n",
       " 'centr',\n",
       " 'colleg',\n",
       " 'core']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [c for c in df_nouns_1n_test.columns if c.startswith('c')]\n",
    "words_to_check = df_nouns_1n_test[columns].T.sort_values('Mean', ascending=False)\n",
    "list(words_to_check.index)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T07:20:00.196572Z",
     "iopub.status.busy": "2021-06-03T07:20:00.196323Z",
     "iopub.status.idle": "2021-06-03T07:20:00.288070Z",
     "shell.execute_reply": "2021-06-03T07:20:00.287613Z",
     "shell.execute_reply.started": "2021-06-03T07:20:00.196546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index for 'children' = 91825\n",
      "\n",
      "ORIGINAL:\n",
      "mon project creativity service tennis aim kids workers events machine courses children resources anglesey variety network ynys aid die area workshops cutters grant â£2,461 awards skill order\n",
      "\n",
      "ORDERED LEM/STEM:\n",
      "{'cutter': 0.38, 'anglesey': 0.36, 'kid': 0.3, 'machin': 0.28, 'creativ': 0.26, 'tenni': 0.24, 'varieti': 0.23, 'network': 0.21, 'order': 0.21, 'resourc': 0.2, 'worker': 0.2, 'cours': 0.19, 'workshop': 0.17, 'event': 0.16, 'award': 0.15, 'skill': 0.15, 'servic': 0.14, 'area': 0.14, 'children': 0.13, 'grant': 0.1, 'project': 0.08}\n"
     ]
    }
   ],
   "source": [
    "word_to_analyse = 'children'\n",
    "idx_df = df_nouns_1n_test[word_to_analyse].iloc[:-1].sort_values(ascending=False)\n",
    "idx = idx_df[idx_df>0].sample(1).index[0]\n",
    "print(f\"index for '{word_to_analyse}' = {idx}\")\n",
    "print()\n",
    "print(f\"ORIGINAL:\\n{corpus2_test.iloc[idx]}\")\n",
    "print()\n",
    "ordered_words = df_nouns_1n_test.iloc[idx][df_nouns_1n_test.iloc[idx]>0].sort_values(ascending=False)\n",
    "scores = {i: round(v, 2) for i, v in zip(list(ordered_words.index), list(ordered_words.values))}\n",
    "print(f\"ORDERED LEM/STEM:\\n{scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Grantnav 10k Predictor",
   "language": "python",
   "name": "grantnav_10k_predictor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
